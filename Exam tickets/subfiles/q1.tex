\documentclass[../document.tex]{subfiles}

\begin{document}
\section{Модель множественной регрессии. Основные предположения регрессионной модели. Оценка коэффициентов методом наименьших квадратов}
	\subsection{Модель множественной линейной регресси}
	\par Модель множественной линейной регрессии предназначена для проверки и изучения связи между одной зависимой переменной и несколькими независимыми  переменными. Предполагается, что такая связь теоретически может быть описана  функцией вида: $Y = \beta_1x_1 + \beta_2x_2 + ... + \beta_kx_k + U$
	
	\subsection{Основные предположения регрессионной модели}
	\begin{enumerate}
		\item $y = X\beta + \epsilon$ - линейная спецификация модели
		\item $X$ - детерменированная матрица максмального ранга $k$
		\item \begin{enumerate}
			\item $E(\mathcal{E}) = 0, V(\mathcal{E}) = E(\mathcal{E}^T\mathcal{E}) = \sigma^2I_n$
			\item $\mathcal{E} \sim N(0, \sigma^2I_n)$
		\end{enumerate}
	\end{enumerate}
	
	\subsection{Оценка коэффициентов методом наименьших квадратов}
	\par Метод наименьших квадратов (МНК) — математический метод, применяемый для решения различных задач, основанный на минимизации суммы квадратов отклонений некоторых функций от экспериментальных входных данных
	\par Пусть регрессионная зависимость является линейной:
	\par $y = \sum\limits_{i=1}^k (b_ix_i + \mathcal{E})$
	\par Пусть $y$ — вектор-столбец наблюдений, а $X$  — это $(n\times k)$ -матрица наблюдений. Матричное представление линейной модели имеет вид:
	\par $y = Xb + \mathcal{E}$
	\par Тогда вектор оценок объясняемой переменной и вектор остатков регрессии будут равны:
	\par $\hat{y} = Xb, e = y - \hat{y} = y - Xb$
	\par соответственно сумма квадратов остатков регрессии будет равна 
	\par $RSS = e^Te = (y - Xb)^T(y-Xb)$
	\par Дифференцируя эту функцию по вектору параметров $b$ и приравняв производные к нулю, получим систему уравнений (в матричной форме):
	\par $(X^TX)b = X^Ty$
\end{document}